---
title: "DigitalCorpsDataTaskCode"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
### Importing libraries 
library(dplyr, warn.conflicts = FALSE)
library(tidyr)
library(tidyverse)
library(tidylog, warn.conflicts = FALSE)
library(visdat)
library(naniar)
library(cdlTools)
library(pROC)
library(caret)
library(e1071)
library(Boruta)
library(Rcpp)
library(randomForest)
library(ROSE)
library(rpart)
library(ggcorrplot)
```
# Section 1: Cleaning/EDA
# Importing and Merging Data 
```{r, results='hide'}
numeric <- read_csv("Data/data_numeric.csv")
categorical <- read_csv("Data/data_categorical.csv")
ordinal <- read_csv("Data/data_ordinal.csv")

num_ord <- left_join(numeric, ordinal, "PERSONID")
num_ord_cat <- left_join(num_ord, categorical, "PERSONID")
```
# Run data quality checks on columns that look to be identical
```{r, results='hide'}
data1 <- num_ord_cat %>%
  select(starts_with("DIABET") | starts_with("MARITAL")) %>%
  mutate(DIABET3_diff = ifelse(DIABETE3.x == DIABETE3.y & 
                                 DIABETE3.x == `DIABETE3...2` &
                                 DIABETE3.x == `DIABETE3...20`, 0, 1)) %>%
  mutate(MARITAL_DIFF = ifelse((`MARITAL...8` == `MARITAL...23`) | 
                                 (is.na(`MARITAL...8`) & is.na(`MARITAL...23`)), 0 ,1)) 
```
# Take out duplicate columns and clean up variable names using janitor
```{r, results='hide'}
data2 <- num_ord_cat %>%
  select(-c(DIABETE3.y, `DIABETE3...2`, `DIABETE3...20`, `MARITAL...8`)) %>%
  janitor::clean_names()

```
# Account for NA and 0 values in data as provided by data documentation
```{r, results='hide'}
data3 <- data2 %>%
  replace_with_na(replace = list(weight2 = c(9999, 7777),
                                 children = 99, 
                                 drvisits = c(77, 99),
                                 income2 = c(77, 99),
                                 checkup1 = c(7, 9), 
                                 sleptim1 = c(77, 99), 
                                 menthlth = c(77, 99), 
                                 diabete3_x = c(7, 9), 
                                 race = c(6, 9), 
                                 flushot6 = c(7, 9), 
                                 employ1 = 9, 
                                 marital_23 = 9, 
                                 cvdcrhd4 = c(7, 9), 
                                 hlthcvr1 = c(77, 99), 
                                 chckidny = c(7, 9), 
                                 useequip = c(7, 9),
                                 totinda = 9, 
                                 addepev2 = c(7, 9),
                                 renthom1 = c(7, 9),
                                 exerany2 = c(7, 9),
                                 blind = c(7, 9),
                                 decide = c(7, 9),
                                 hlthpln1 = c(7, 9), 
                                 genhlth = c(7, 9), 
                                 asthma3 = c(7, 9)))%>%
  mutate(children = replace(children, children == 88, 0)) %>%
  mutate(drvisits = replace(drvisits, drvisits == 88, 0)) %>%
  mutate(menthlth = replace(menthlth, menthlth == 88, 0)) %>%
  mutate(checkup1 = replace(checkup1, checkup1 == 88, 0)) %>%
  mutate(marital = marital_23) %>%
  mutate(diabete3 = diabete3_x) %>%
  select(-c(marital_23, diabete3_x))
```
# The documentation specifies that some weight values are in kilograms-- convert all of them to pounds 
```{r, results='hide'}
data3_cleanweight1 <- data3 %>%
  mutate(
    weight_correct = case_when(
      nchar(as.character(weight2))==4 ~ as.numeric(str_sub(weight2, -3)) * 2.20462,
      TRUE ~ weight2
    )) %>%
  select(-weight2)
```
# The data currently uses state fips codes instead of abbreviations. Abbreviations will make the data and results easier to interpret. 
```{r, results='hide'}
data3_cleanweight1$state <- fips(data3_cleanweight1$state, to="Abbreviation")
```

# Check for unique ID's 
```{r, results='hide'}
uniqueidcheck <- data3_cleanweight1 %>% 
  group_by(personid) %>% 
  mutate(duplicate_name = n()-1)
# All ID's are unique! 
```

# Check missingness of data using vis_dat-- sorts columns according to types of data
```{r, results='hide'}
vis_dat(data3_cleanweight1)
```

# Section 2: Model Selection 
## Demographic Factors
##### BMI- bmi5cat
##### Age - ageg5yr
##### Gender - sex
##### Race  - race 

## Social Factors
##### Stress - menthlth
##### Depression - addepev2

## Lifestyle Factors
##### Smoking - smoker3
##### Physical exercise - totinda
##### Sleep - sleptim1

## Medical History 
##### Diagnosed with coronary heart disease or angina  - cvdcrhd4
##### Ever told you have kidney disease - chckidny

# Section 3: Preparing data for modeling
```{r, results='hide'}
data4 <- data3_cleanweight1 %>%
  select(personid, ageg5yr, bmi5cat, sex, race, 
         addepev2, menthlth, cvdcrhd4, chckidny, 
         totinda, smoker3, diabete3, sleptim1) %>%
  filter(ageg5yr > 2) %>%
  filter(diabete3 == 1 | diabete3 == 3) %>%
  mutate(diabete3 = case_when(diabete3 == 3 ~ 0, 
         TRUE ~ 1)) %>%
  na.omit()
```
# Check racial breakdown of data and assess if it is representative of nation
```{r, results='hide'}
data4_sample <- data4 %>%
  group_by(race) %>%
  mutate(Freq=n()) %>%
  mutate(perc_total_respondents = Freq/3808*100) %>%
  # Compare perc_total_respondents with actual national racial breakdown using https://www.census.gov/content/dam/Census/library/publications/2015/demo/p25-1143.pdf page 9
  # There are a disproportionate number of white respondents, which could potentially skew the sample 
  # Let's use weights to fix for heteroskedasticity AFTER establishing the training and test models because the representative-ness will change based on the random sample
  ungroup()
```
# The following variables are categorical or ordinal. We need to convert the following objects to factors to represent that. 
```{r, results='hide'}
data4_sample$diabete3 <- as.numeric(data4_sample$diabete3)
data4_sample$race <- factor(data4_sample$race)
data4_sample$sex <- factor(data4_sample$sex)
data4_sample$ageg5yr <- factor(data4_sample$ageg5yr)
data4_sample$bmi5cat <- factor(data4_sample$bmi5cat)
data4_sample$smoker3 <- factor(data4_sample$smoker3)
data4_sample$addepev2 <- factor(data4_sample$addepev2)
data4_sample$cvdcrhd4 <- factor(data4_sample$cvdcrhd4)
data4_sample$chckidny <- factor(data4_sample$chckidny)
data4_sample$totinda <- factor(data4_sample$totinda)
```
# Before we start modeling, let's check the distribution of diabetes 
```{r, results='hide'}
prop.table(table(data4_sample$diabete3))
# Only ~14% of the data captures individuals diagnosed with diabetes. When we're developing models, this imbalance will skew towards not being diagnosed with diabetes, which could lead to false results because does have enough data to learn about the minority.
```

# Split the data into training and test sets before adjusting the samples. We use the training data to test and validate the model then treat the test dataset as new data to evaluate how effective the model is. We will adjust the racial representative-ness of the sample in the training data. 
```{r, results='hide'}
train <- data4_sample %>% 
  dplyr::sample_frac(0.75) %>%
  group_by(race) %>%
  mutate(Freq=n()) %>%
  mutate(perc_total_respondents = Freq/3808*100) %>%
  mutate(perc_population = case_when(race == 1 ~  77.5,
                                     race == 2 ~ 13.2,
                                     race == 3 ~ 1.2, 
                                     race == 4 ~ 5.4, 
                                     race == 5 ~ 0.2, 
                                     race == 7 ~ 2.5, 
                         TRUE~15.3)) %>%
  ## Used https://www.census.gov/content/dam/Census/library/publications/2015/demo/p25-1143.pdf page 9
  ## subtracted 2.1 from the hispanic because we have 17.4 percent hispanic and that is not ONLY hispanic
  mutate(race_weight = perc_population/perc_total_respondents) %>%
  select(-c(perc_population, perc_total_respondents, Freq)) %>%
  ungroup()
prop.table(table(train$diabete3))

# Creating test sample-- still need to create a racially representative sample here as well before testing the model we created
test  <- dplyr::anti_join(data4_sample, train, by = 'personid') %>%
  group_by(race) %>%
  mutate(Freq=n()) %>%
  mutate(perc_total_respondents = Freq/3808*100) %>%
  mutate(perc_population = case_when(race == 1 ~  77.5,
                                     race == 2 ~ 13.2,
                                     race == 3 ~ 1.2, 
                                     race == 4 ~ 5.4, 
                                     race == 5 ~ 0.2, 
                                     race == 7 ~ 2.5, 
                         TRUE~15.3)) %>%
  ## Used https://www.census.gov/content/dam/Census/library/publications/2015/demo/p25-1143.pdf page 9
  ## subtracted 2.1 from the hispanic because we have 17.4 percent hispanic and that is not ONLY hispanic
  mutate(race_weight = perc_population/perc_total_respondents) %>%
  select(-c(perc_population, perc_total_respondents, Freq)) %>%
  ungroup()
prop.table(table(test$diabete3))
```
# Now that the data is split, we will create samples for both over and under sampled data 
```{r, results='hide'}
# Try oversampling first 
data_balanced_over_train <- ovun.sample(diabete3 ~ ., data = train, method = "over")$data
# We replicate the observations from minority class (individuals with diabetes) to balance the data
prop.table(table(data_balanced_over_train$diabete3))

# Now undersampling  
data_balanced_under_train <- ovun.sample(diabete3 ~ ., data = train, method = "under")$data
# We reduce the number of observations from majority class (individuals without diabetes) to balance the data 
prop.table(table(data_balanced_under_train$diabete3))
```
# Section 4: Deploying the logit model 
## Now that we have the following: 
### Clean data
### Training/test sets,
## Non-sampled, Over-sampled, and Under-sampled datasets. 
## We can use a logit model to predict if someone will be diagnosed with diabetes. Start with the non-sampled data.
## Run the regression and examine the results 
```{r, results='hide'}
mylogit <- glm(diabete3 ~.-personid-race_weight, data = train, family = binomial(link = "logit"), weights = race_weight)

anova_logit <- anova(mylogit, test="Chisq")
# Use a chi-squared test because we have a lot of categorical and ordinal variables. The p values we have in the regression output correspond to individual Wald tests, but those p values do not tell us if the entire categorical/ordinal variable) explains the probability of getting diabetes. We have to test the coefficients together, so we perform a deviance goodness of fit test. 
```
# View regression output
```{r}
summary(mylogit)

anova_logit
# How to interpret this test: rejecting the null means the reduction in deviance is larger than one would expect if a variable did not explain anything about the outcome
```
## Make predictions using test data
```{r, results='hide'}
probabilities <- mylogit %>% predict(test, type = "response")
# The result is continuous values of probabilities of diabetes occurrence, but we are interested in predicting if someone will have diabetes or not. If anyone has a probability above 50%, we assume that they will be diagnosed with diabetes.
predicted_classes <- ifelse(probabilities > 0.5, 1, 0)
# Take these predictions and append as a column to the test dataframe 
# Do do that, we must first converting predicted classes to a dataframe
predicted_classes_logit <- as.data.frame(predicted_classes)
test_predicted_logit <- cbind(predicted_classes_logit, test) 
```
## Evaluate the model by creating a confusion matrix 
```{r, results='hide'}
logitconfusionmatrix <-confusionMatrix(data=as.factor(test_predicted_logit$predicted_classes), reference=as.factor(test_predicted_logit$diabete3))
```
```{r}
logitconfusionmatrix
```
## Create ROC Curve to calculate the performance of the classification model
```{r}
roc_object <- roc(test$diabete3, probabilities)
# Calculate area under curve, the closer this is to 1 the more useful the model is
auc_logit <- auc(roc_object) 
auc_logit
```
# Logistic Regression using Oversampled Data 
## Run the regression and examine the results 
```{r, results='hide'}
mylogit_oversample <- glm(diabete3 ~.-race_weight-personid, data = data_balanced_over_train, family = binomial(link = "logit"), weights = race_weight)
anova_logit_over <- anova(mylogit_oversample, test="Chisq")
```
# View regression output
```{r}
summary(mylogit_oversample)

anova_logit_over
```
## Make predictions using test data
```{r, results='hide'}
probabilities_logit_over <- mylogit_oversample %>% predict(test, type = "response")
predicted_classes_over <- ifelse(probabilities_logit_over > 0.5, 1, 0)
# Take these and append as columns to the test dataframe 
predicted_classes_logit_over <- as.data.frame(predicted_classes_over)
test_predicted_logit_over <- cbind(predicted_classes_logit_over, test) 
```
## Evaluate the model by creating a confusion matrix 
```{r, results='hide'}
logitconfusionmatrix_over <- confusionMatrix(data=as.factor(test_predicted_logit_over$predicted_classes_over), reference=as.factor(test_predicted_logit_over$diabete3))
```
```{r}
logitconfusionmatrix_over
```
## Create ROC Curve to calculate the performance of the classification model
```{r}
roc_object_over <- roc(test$diabete3, probabilities_logit_over)
# calculate area under curve
auc_logit_over <- auc(roc_object)
auc_logit_over
```
# Logistic Regression using Undersampled Data 
## Run the regression and examine the results 
```{r, results='hide'}
mylogit_undersample <- glm(diabete3 ~.-race_weight-personid, data = data_balanced_under_train, family = binomial(link = "logit"), weights = race_weight)
anova_logit_under <- anova(mylogit_undersample, test="Chisq")
```
# View regression output
```{r}
summary(mylogit_undersample)

anova_logit_under
```
## Make predictions using test data
```{r, results='hide'}
probabilities_logit_under <- mylogit_undersample %>% predict(test, type = "response")
predicted_classes_under <- ifelse(probabilities_logit_under > 0.5, 1, 0)
# Take these and append as columns to the test dataframe 
predicted_classes_logit_under <- as.data.frame(predicted_classes_under)
test_predicted_logit_under <- cbind(predicted_classes_logit_under, test) 
```
## Evaluate the model by creating a confusion matrix 
```{r, results='hide'}
logitconfusionmatrix_under <- confusionMatrix(data=as.factor(test_predicted_logit_under$predicted_classes_under),                                  reference=as.factor(test_predicted_logit_under$diabete3))
```
```{r}
logitconfusionmatrix_under
```
## Create ROC Curve to calculate the performance of the classification model
```{r}
roc_object_under <- roc(test$diabete3, probabilities_logit_under)
# calculate area under curve
auc_logit_under <- auc(roc_object) 
auc_logit_under
```
## Takeaways on sampling in logit 
##### Overall, using the over or undersampled data doesn't lead to different enough results to prefer either one over the non-sampled data in the currently specificed model.
##### Because the logit model initially performs well on the overfit data but not on the test data (as evidenced by low accuracy), we conclude that the oversampled model is overfit. 
##### It is also worth noting that the usefulness of the over, under, and non-sampled models is the same. This is because changing the class ratio only affects the intercept of a standard logistic regression model. It is a monotonic transformation of the log-odds. This also represents a monotonic function of the predicted probability values. Monotonic functions preserve order, and AUC stays the same with order meaning the AUC does not change.
##### In all of the logit models, the auc indicates that the model's ability to dsitinguish between outcomes is fair. Given that over and under sampling didn't make much of a difference, it is likely that the model itself is incorrently specified. 
##### Given the time constraints, I chose regressors based on commonly cited empirical literature. If I had more time, I would try variations of this model with different regressors or use Mallows CP and compare confusion matrices and AUC's to see which factors are most significant. 





